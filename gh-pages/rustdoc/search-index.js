var searchIndex = JSON.parse('{\
"minc":{"doc":"The Minimal programming language compiler binary.","t":"F","n":["main"],"q":[[0,"minc"]],"d":[""],"i":[0],"f":[[[]]],"c":[],"p":[]},\
"minimal_compiler":{"doc":"The Minimal programming language compiler.","t":"AGDLLLLLLLLLLLAALLLNNENNNNNNDENNNLLLLLLLLLLLLLLLAALLLLLLLLLALLLMAALLLAMLLLLLLLLLLLLMDMLLLLLMMLLLLLLLLLLNNENNNNNNNNNLLLLLLLLLLLLLLLDLLLLLLLLLLLLLLLMENNNDNDMLLLLLLLLLLLLLLLLLLMLLLMLLLMLLLLLLMLLLLLLLLLLLLLLNNNNNNENNNNLLLLLLLLLLLLLLLDLLLLLLLLMLLMLLLLNNNIELLLLLLLLLLLKLLLMMM","n":["tokenizer","InputTextIter","Tokenizer","borrow","borrow_mut","clone","clone_into","fmt","from","into","into_iter","new","next","to_owned","token","tokenize","try_from","try_into","type_id","Comment","Delim","Error","Error","Ident","NoNumberAfterBase","Number","Operator","String","Token","TokenValue","UnterminatedBlockComment","UnterminatedStringLiteral","Whitespace","borrow","borrow","borrow","borrow_mut","borrow_mut","borrow_mut","clone","clone","clone","clone_into","clone_into","clone_into","cmp","cmp","cmp","comment","delim","eq","eq","eq","fmt","fmt","fmt","from","from","from","ident","into","into","into","lexeme","literal","operator","partial_cmp","partial_cmp","partial_cmp","span","span","to_owned","to_owned","to_owned","try_from","try_from","try_from","try_into","try_into","try_into","type_id","type_id","type_id","value","Comment","block","borrow","borrow_mut","clone","clone_into","cmp","content","doc","eq","fmt","from","into","partial_cmp","to_owned","tokenize","try_from","try_into","type_id","Colon","Comma","Delim","Dot","Hash","LBrace","LBracket","LParen","RBrace","RBracket","RParen","SemiColon","borrow","borrow_mut","clone","clone_into","cmp","eq","fmt","from","into","partial_cmp","to_owned","tokenize","try_from","try_into","type_id","Ident","borrow","borrow_mut","clone","clone_into","cmp","eq","fmt","from","into","partial_cmp","to_owned","tokenize","try_from","try_into","type_id","value","Base","Binary","Decimal","Hexadecimal","Number","Octal","String","base","borrow","borrow","borrow","borrow_mut","borrow_mut","borrow_mut","clone","clone","clone","clone_into","clone_into","clone_into","cmp","cmp","cmp","eq","eq","eq","exponent","fmt","fmt","fmt","fract_part","from","from","from","int_part","into","into","into","partial_cmp","partial_cmp","partial_cmp","s","to_owned","to_owned","to_owned","tokenize","tokenize","try_from","try_from","try_from","try_into","try_into","try_into","type_id","type_id","type_id","Ampersand","Asterisk","At","Bang","Equal","Minus","Operator","Percent","Plus","QuestionMark","Slash","borrow","borrow_mut","clone","clone_into","cmp","eq","fmt","from","into","partial_cmp","to_owned","tokenize","try_from","try_into","type_id","Span","borrow","borrow_mut","clone","clone_into","cmp","eq","fmt","from","from","into","partial_cmp","to","to_owned","try_from","try_into","type_id","Eof","NoMatch","Token","Tokenize","TokenizeResult","borrow","borrow_mut","clone","clone_into","cmp","eq","fmt","from","into","partial_cmp","to_owned","tokenize","try_from","try_into","type_id","lexeme","span","value"],"q":[[0,"minimal_compiler"],[1,"minimal_compiler::tokenizer"],[19,"minimal_compiler::tokenizer::token"],[84,"minimal_compiler::tokenizer::token::comment"],[103,"minimal_compiler::tokenizer::token::delim"],[130,"minimal_compiler::tokenizer::token::ident"],[147,"minimal_compiler::tokenizer::token::literal"],[203,"minimal_compiler::tokenizer::token::operator"],[229,"minimal_compiler::tokenizer::token::span"],[246,"minimal_compiler::tokenizer::tokenize"],[266,"minimal_compiler::tokenizer::tokenize::TokenizeResult"]],"d":["The Minimal programming language lexical analyzer.","The type of the <code>Tokenizer</code>’s input iterator.","The Minimal language tokenizer.","","","","","","Returns the argument unchanged.","Calls <code>U::from(self)</code>.","","Creates a new tokenizer with specified input.","Gets the next token.","","A token, output of the tokenizer, input of the lexer (not …","The trait and result type for tokenizing text.","","","","A comment.","A delimiter, like brackets and colons.","A lexical analysis error.","A tokenization error.","An indentifier or keyword.","No number after base prefix in number literal.","A number literal (integer or floating point).","An operator, like <code>+</code>, <code>=</code>, <code>?</code>, <code>!</code>.","A string literal.","A token, output of the tokenizer, input of the lexer.","The value of a token.","Unterminated block comment.","An unterminated string literal.","A whitespace character.","","","","","","","","","","","","","","","","A comment token, line or block, doc or regular, not …","The module for delimiter (e.g. semicolons, commas, …","","","","","","","Returns the argument unchanged.","Returns the argument unchanged.","Returns the argument unchanged.","The module for identfier tokens, including keywords.","Calls <code>U::from(self)</code>.","Calls <code>U::from(self)</code>.","Calls <code>U::from(self)</code>.","The whole token as a string","The module for literals, like a number or a string.","The module for operator (e.g. <code>+</code>, <code>?</code>, <code>=</code>, <code>!</code>) tokens.","","","","The span of a token.","The <code>Span</code> of the token.","","","","","","","","","","","","","The value of the token of an enum variant.","A comment, line or block, doc or regular, not recusive.","Whether or not the comment is a block comment (<code>/* */</code>).","","","","","","The content of the comment.","Whether or not the comment is a documentation comment.","","","Returns the argument unchanged.","Calls <code>U::from(self)</code>.","","","","","","","<code>:</code>","<code>,</code>","A delimiter (e.g. semicolons, commas, brackets) token.","<code>.</code>","<code>#</code>","<code>{</code>","<code>[</code>","<code>(</code>","<code>}</code>","<code>]</code>","<code>)</code>","<code>;</code>","","","","","","","","Returns the argument unchanged.","Calls <code>U::from(self)</code>.","","","","","","","An identifier, or keyword.","","","","","","","","Returns the argument unchanged.","Calls <code>U::from(self)</code>.","","","","","","","The value of the ident, same as the lexeme.","A base/radix of a number.","Binary (base 2)","Decimal (base 10)","Hexadecimal (base 16)","A number token, represents any integer/float.","Octal (base 8)","A string token value.","The base/radix of the number.","","","","","","","","","","","","","","","","","","","The exponent of the number.","","","","The fractional part of the number.","Returns the argument unchanged.","Returns the argument unchanged.","Returns the argument unchanged.","The integer part of the number.","Calls <code>U::from(self)</code>.","Calls <code>U::from(self)</code>.","Calls <code>U::from(self)</code>.","","","","The unescaped value under the string.","","","","","","","","","","","","","","","<code>&amp;</code>","<code>*</code>","<code>@</code>","<code>!</code>","<code>=</code>","<code>-</code>","An operator (e.g. ‘+’, ‘?’, ‘!’) token.","<code>%</code>","<code>+</code>","<code>?</code>","<code>/</code>","","","","","","","","Returns the argument unchanged.","Calls <code>U::from(self)</code>.","","","","","","","The span of a token (inclusive range).","","","","","","","","Returns the argument unchanged.","The start of a token in text.","Calls <code>U::from(self)</code>.","","The end of a token in text.","","","","","End of input, returned if the attempt to get the first …","Text didn’t match, iterator wasn’t advanced.","A token, returned if text matched.","Trait for tokenizing text.","Result type for the <code>tokenize</code> function from the <code>Tokenize</code> …","","","","","","","","Returns the argument unchanged.","Calls <code>U::from(self)</code>.","","","Tokenizes text.","","","","The whole token as a character array.","The <code>Span</code> of the token.","The value of the token as an enum variant."],"i":[0,0,0,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,8,8,0,8,8,9,8,8,8,0,0,9,9,8,7,8,9,7,8,9,7,8,9,7,8,9,7,8,9,0,0,7,8,9,7,8,9,7,8,9,0,7,8,9,7,0,0,7,8,9,0,7,7,8,9,7,8,9,7,8,9,7,8,9,7,0,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,15,15,0,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,0,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,0,17,17,17,0,17,0,18,17,18,19,17,18,19,17,18,19,17,18,19,17,18,19,17,18,19,18,17,18,19,18,17,18,19,18,17,18,19,17,18,19,19,17,18,19,18,19,17,18,19,17,18,19,17,18,19,20,20,20,20,20,20,0,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,0,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,21,14,14,14,0,0,14,14,14,14,14,14,14,14,14,14,14,22,14,14,14,23,23,23],"f":[0,0,0,[[]],[[]],[1,1],[[]],[[1,2],3],[[]],[[]],[[]],[[],1],[1,4],[[]],0,0,[[],5],[[],5],[[],6],0,0,0,0,0,0,0,0,0,0,0,0,0,0,[[]],[[]],[[]],[[]],[[]],[[]],[7,7],[8,8],[9,9],[[]],[[]],[[]],[[7,7],10],[[8,8],10],[[9,9],10],0,0,[[7,7],11],[[8,8],11],[[9,9],11],[[7,2],3],[[8,2],3],[[9,2],3],[[]],[[]],[[]],0,[[]],[[]],[[]],0,0,0,[[7,7],[[4,[10]]]],[[8,8],[[4,[10]]]],[[9,9],[[4,[10]]]],0,0,[[]],[[]],[[]],[[],5],[[],5],[[],5],[[],5],[[],5],[[],5],[[],6],[[],6],[[],6],0,0,0,[[]],[[]],[12,12],[[]],[[12,12],10],0,0,[[12,12],11],[[12,2],3],[[]],[[]],[[12,12],[[4,[10]]]],[[]],[13,14],[[],5],[[],5],[[],6],0,0,0,0,0,0,0,0,0,0,0,0,[[]],[[]],[15,15],[[]],[[15,15],10],[[15,15],11],[[15,2],3],[[]],[[]],[[15,15],[[4,[10]]]],[[]],[13,14],[[],5],[[],5],[[],6],0,[[]],[[]],[16,16],[[]],[[16,16],10],[[16,16],11],[[16,2],3],[[]],[[]],[[16,16],[[4,[10]]]],[[]],[13,14],[[],5],[[],5],[[],6],0,0,0,0,0,0,0,0,0,[[]],[[]],[[]],[[]],[[]],[[]],[17,17],[18,18],[19,19],[[]],[[]],[[]],[[17,17],10],[[18,18],10],[[19,19],10],[[17,17],11],[[18,18],11],[[19,19],11],0,[[17,2],3],[[18,2],3],[[19,2],3],0,[[]],[[]],[[]],0,[[]],[[]],[[]],[[17,17],[[4,[10]]]],[[18,18],[[4,[10]]]],[[19,19],[[4,[10]]]],0,[[]],[[]],[[]],[13,14],[13,14],[[],5],[[],5],[[],5],[[],5],[[],5],[[],5],[[],6],[[],6],[[],6],0,0,0,0,0,0,0,0,0,0,0,[[]],[[]],[20,20],[[]],[[20,20],10],[[20,20],11],[[20,2],3],[[]],[[]],[[20,20],[[4,[10]]]],[[]],[13,14],[[],5],[[],5],[[],6],0,[[]],[[]],[21,21],[[]],[[21,21],10],[[21,21],11],[[21,2],3],[[]],0,[[]],[[21,21],[[4,[10]]]],0,[[]],[[],5],[[],5],[[],6],0,0,0,0,0,[[]],[[]],[14,14],[[]],[[14,14],10],[[14,14],11],[[14,2],3],[[]],[[]],[[14,14],[[4,[10]]]],[[]],[13,14],[[],5],[[],5],[[],6],0,0,0],"c":[],"p":[[3,"Tokenizer"],[3,"Formatter"],[6,"Result"],[4,"Option"],[4,"Result"],[3,"TypeId"],[3,"Token"],[4,"TokenValue"],[4,"Error"],[4,"Ordering"],[15,"bool"],[3,"Comment"],[6,"InputTextIter"],[4,"TokenizeResult"],[4,"Delim"],[3,"Ident"],[4,"Base"],[3,"Number"],[3,"String"],[4,"Operator"],[3,"Span"],[8,"Tokenize"],[13,"Token"]],"a":{"radix":[147]}}\
}');
if (typeof window !== 'undefined' && window.initSearch) {window.initSearch(searchIndex)};
if (typeof exports !== 'undefined') {exports.searchIndex = searchIndex};
